<section class="section experiences-section">
                <h2 class="section-title"><i class="fa fa-briefcase"></i>Work Experience</h2>
  
                  <div class="item">
                    <div class="meta">
                        <div class="upper-row">
                            <h3 class="job-title">Data Engineer Intern</h3>
                            <div class="time">June 2022 - September 2022</div>
                        </div><!--//upper-row-->
                        <div class="company">Amazon.com Inc, Seattle, United States</div>
                    </div><!--//meta-->
                    <div class="details">
                      
<div style="text-align: justify; text-justify:inter-word"> 
  <p>I worked with the HR organization's data warehouse team. This team is a central source for all HR-related data on Amazon Redshift, 
	 processing a high volume of computation. As an intern, I built a dashboard that captures the Redshift system performance metrics 
	 enabling the team to dive deep into the root causes of recurring issues. By solving these issues, the team will provide a smooth querying performance 
	 for downstream users, showcasing customer obsession, an Amazon leadership principle every employee strives to follow.  </p>

<p>I organized 1:1 meetings with Data Engineers to learn from their experiences and challenges. These meetings were insightful, 
  showcasing how Data Engineering is at Amazon, and I observed some common challenges. Hence, I proposed to include Data Engineers'
  workload metrics in the Redshift performance dashboard enabling the team to visualize team-specific data at a single source. 
  These metrics would streamline and improve the processing of customer requests maintaining workload balance amongst team members.</p> </div>

                    </div><!--//details-->
                </div><!--//item-->
                
                <div class="item">
                    <div class="meta">
                        <div class="upper-row">
                            <h3 class="job-title">Graduate Analyst (ETL Developer)</h3>
                            <div class="time">July 2018 - July 2021</div>
                        </div><!--//upper-row-->
                        <div class="company">Barclays Global Service Centre Private Limited, Pune, India</div>
                    </div><!--//meta-->
                    <div class="details">
                      
<div style="text-align: justify; text-justify:inter-word"> 
  <p>I worked with the Data Delivery Services Team as an Extract Transform and Load (ETL) Developer. 
  Data processing was done using the tool Ab Initio on top of the Hadoop Infrastructure. 
  I got to work with Parquet and Avro files and analyzed data in HIVE using SQL. 
  We had to collaborate with Data Consumers to understand their requirements and build a pipeline (Ab Initio graphs) to load data to the warehouse. 
  Our data was used for Business Analytics and Machine Learning model training and hence any discrepancy would lead to invalid results. 
  To maintain data quality, we would perform data reconciliation and validate data using SQL. </p>

<p>The ETL jobs were automated and I got to learn about job scheduling, resource management, and handling dependencies. 
  In addition to that, I got experience working with UNIX commands and how to utilize Shell scripting to automate trivial tasks. 
  Lastly, my affinity towards data made me initiate a biweekly meeting with Data Stewards to understand the relationship and meaning of underlying data,
  how to best perform transformations based on use cases and how we could maintain it for the most optimal use.</p> </div>

                    </div><!--//details-->
                </div><!--//item-->
                
                
            </section><!--//section-->
